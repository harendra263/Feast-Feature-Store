{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import calendar\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"rideshare_kaggle.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df = df.rename(columns = {\"datetime\": \"event_timestamp\"})\n",
    "\n",
    "df[\"price\"] = df[\"price\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_len = len(df)\n",
    "# ids = [x for x in range(0, data_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Driver_Id\"] = ids\n",
    "\n",
    "# df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_cols = ['apparentTemperature', 'precipIntensity', 'precipProbability', 'humidity', 'windSpeed',\n",
    "            'windGust', 'windGustTime', 'visibility', 'temperatureHigh', 'timezone',\n",
    "            'temperatureHighTime', 'temperatureLow', 'temperatureLowTime',\n",
    "            'apparentTemperatureHigh', 'apparentTemperatureHighTime',\n",
    "                'apparentTemperatureLowTime', 'icon',\n",
    "            'dewPoint', 'pressure', 'uvIndex','visibility.1', 'ozone', 'sunriseTime', 'sunsetTime', 'moonPhase',\n",
    "            'precipIntensityMax', 'uvIndexTime', \n",
    "            'temperatureMinTime', 'temperatureMaxTime',\n",
    "            'apparentTemperatureMinTime', 'apparentTemperatureMaxTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(extra_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_week = [calendar.day_name[x.dayofweek] for x in df[\"event_timestamp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['destination'] = label_encoder.fit_transform(df['destination'])\n",
    "df['product_id'] = label_encoder.fit_transform(df['product_id'])\n",
    "df['short_summary']= label_encoder.fit_transform(df['short_summary'])\n",
    "df['long_summary']= label_encoder.fit_transform(df['long_summary'])\n",
    "df['name'] = label_encoder.fit_transform(df['name'])\n",
    "df['source'] = label_encoder.fit_transform(df['source'])\n",
    "df['cab_type'] = label_encoder.fit_transform(df['cab_type'])\n",
    "df['name'] = label_encoder.fit_transform(df['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_df = df.loc[:, df.columns != \"price\"]\n",
    "target_df = df[[\"id\",\"event_timestamp\",\"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do Feast Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!feast init feast_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_df.to_parquet(path=\"feast_demo/feature_repo/data/predictor_df.parquet\")\n",
    "target_df.to_parquet(path=\"feast_demo/feature_repo/data/target_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"feast_demo/feature_repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!feast apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "from feast.infra.offline_stores.file_source import SavedDatasetFileStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = FeatureStore(repo_path=\".\")\n",
    "entity_df = pd.read_parquet(path = \"data/target_df.parquet\")\n",
    "\n",
    "\n",
    "training_data = store.get_historical_features(\n",
    "    entity_df= entity_df,\n",
    "    features = [\n",
    "        \"driver_stats:hour\",\n",
    "        \"driver_stats:day\",\n",
    "        \"driver_stats:month\",\n",
    "        \"driver_stats:source\",\n",
    "        \"driver_stats:destination\",\n",
    "        \"driver_stats:cab_type\",\n",
    "        \"driver_stats:product_id\",\n",
    "        \"driver_stats:name\",\n",
    "        \"driver_stats:distance\",\n",
    "        \"driver_stats:surge_multiplier\",\n",
    "        \"driver_stats:latitude\",\n",
    "        \"driver_stats:longitude\",\n",
    "        \"driver_stats:temperature\",\n",
    "        \"driver_stats:short_summary\",\n",
    "        \"driver_stats:apparentTemperatureLow\",\n",
    "        \"driver_stats:windBearing\",\n",
    "        \"driver_stats:cloudCover\",\n",
    "        \"driver_stats:temperatureMin\",\n",
    "        \"driver_stats:temperatureMax\",\n",
    "        \"driver_stats:apparentTemperatureMin\",\n",
    "        \"driver_stats:apparentTemperatureMax\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = store.create_saved_dataset(\n",
    "    from_=training_data, \n",
    "    name=\"Uber-lyft-Dataset\",\n",
    "    storage= SavedDatasetFileStorage(\"data/Uber-Lyft-Dataset.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from joblib import dump\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting feature store\n",
    "\n",
    "store = FeatureStore(repo_path = \".\")\n",
    "\n",
    "# Retrieving the saved dataset and converting to pandas Dataframe\n",
    "\n",
    "training_df = store.get_saved_dataset(name=\"Uber-lyft-Dataset\").to_df()\n",
    "\n",
    "# Seperating the features and labels\n",
    "\n",
    "y = training_df[\"price\"]\n",
    "X = training_df.drop([\"id\", \"event_timestamp\", \"price\"], axis=1)\n",
    "\n",
    "# Splitting the dataset into train and tests\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_models(X_train, X_test, y_train, y_test):\n",
    "    print(\"Liner Regression\")\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model = lr_model.fit(X_train, y_train)\n",
    "    print(\"Linear Regression Score: \", lr_model.score(X_test, y_test))\n",
    "    \n",
    "    print(\"Random Forest Regressor\")\n",
    "    reg = RandomForestRegressor()\n",
    "    reg = reg.fit(X_train, y_train)\n",
    "    print(\"Random Forest Score: \", reg.score(X_test, y_test))\n",
    "    \n",
    "    print(\"Decision Tree Regressor\")\n",
    "    decision = DecisionTreeRegressor()\n",
    "    decision = decision.fit(X_train, y_train)\n",
    "    print(\"Decision Tree Regressor: \", decision.score(X_test, y_test))\n",
    "    \n",
    "    print(\"Gradient Boosting Regressor\")\n",
    "    gbm = GradientBoostingRegressor()\n",
    "    gbm = gbm.fit(X_train, y_train)\n",
    "    print(\"Gradient Boosting Regressor: \", gbm.score(X_test, y_test))\n",
    "    \n",
    "    return lr_model, reg, decision, gbm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = train_test_models(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "dump(value=models[1], filename=\"gbm.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Prepare online feature store\n",
    "(Loading the features to online store)\n",
    "\n",
    "There are two ways you can use to load features to your online store<br>\n",
    "\n",
    "materialize\n",
    "materialize loads the latest features between two dates.\n",
    "\n",
    "feast materialize 2020–01–01T00:00:00 2022–01–01T00:00:00<br>\n",
    "\n",
    "materialize-incremental\n",
    "materialize-incremental loads features up to the provided end date:\n",
    "\n",
    "feast materialize-incremental 2022–01–01T00:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "from datetime import datetime, timedelta\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"event_timestamp\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.now() - datetime(2018, 12, 18, 19, 15, 10, 432493)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.now() -timedelta(days=1867)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime(2018, 12, 18, 19, 15, 10, 432493)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"event_timestamp\"] >= \"2018-12-17\") & (df[\"event_timestamp\"] <= \"2018-12-18\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = FeatureStore(repo_path=\".\")\n",
    "\n",
    "store.materialize(start_date=datetime.utcnow() - timedelta(days=1868), end_date=datetime.utcnow() - timedelta(days=1867))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "store= FeatureStore(repo_path=\".\")\n",
    "\n",
    "\n",
    "feast_features = [\n",
    "        \"driver_stats:hour\",\n",
    "        \"driver_stats:day\",\n",
    "        \"driver_stats:month\",\n",
    "        \"driver_stats:source\",\n",
    "        \"driver_stats:destination\",\n",
    "        \"driver_stats:cab_type\",\n",
    "        \"driver_stats:product_id\",\n",
    "        \"driver_stats:name\",\n",
    "        \"driver_stats:distance\",\n",
    "        \"driver_stats:surge_multiplier\",\n",
    "        \"driver_stats:latitude\",\n",
    "        \"driver_stats:longitude\",\n",
    "        \"driver_stats:temperature\",\n",
    "        \"driver_stats:short_summary\",\n",
    "        \"driver_stats:apparentTemperatureLow\",\n",
    "        \"driver_stats:windBearing\",\n",
    "        \"driver_stats:cloudCover\",\n",
    "        \"driver_stats:temperatureMin\",\n",
    "        \"driver_stats:temperatureMax\",\n",
    "        \"driver_stats:apparentTemperatureMin\",\n",
    "        \"driver_stats:apparentTemperatureMax\"\n",
    "]\n",
    "\n",
    "feature_vector = store.get_online_features(\n",
    "    features=feast_features, \n",
    "    entity_rows=[\n",
    "        {\"Driver_Id\":\"2effa2c2-6728-4274-b904-199a9fc830c4\"},\n",
    "        {\"Driver_Id\": \"1d451059-895c-4179-8cec-40adfbc4f6d3\"}\n",
    "                ]\n",
    ").to_dict()\n",
    "\n",
    "# Converting the features to a DataFrame\n",
    "features_df = pd.DataFrame.from_dict(data=feature_vector)\n",
    "\n",
    "pprint(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the predict function to see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our model and doing inference\n",
    "reg = load(\"gbm.joblib\")\n",
    "predictions = reg.predict(features_df[sorted(features_df.drop(\"id\", axis=1))])\n",
    "print(predictions)\n",
    "prediction_probabilities = reg.predict(features_df[sorted(features_df.drop(\"id\", axis=1))])\n",
    "print(prediction_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eliminate(trained_model, X, y, n_features: int = 40):\n",
    "    rfe = RFE(trained_model, n_features_to_select=n_features)\n",
    "    rfe = rfe.fit(X, y)\n",
    "    X_new = X[X.columns[rfe.support_]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=0)\n",
    "    new_fit = trained_model.fit(X_train, y_train)\n",
    "    print(new_fit.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [5, 10, 15, 20]\n",
    "\n",
    "for model in models:\n",
    "    for nf in n_features:\n",
    "        print(f'{model}->{nf} features')\n",
    "        feature_eliminate(model, X, y, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
